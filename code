import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np
import cv2
import os
from sklearn.model_selection import train_test_split
from google.colab import drive

def mount_drive():
    try:
        drive.mount('/content/drive', force_remount=True)
    except Exception as e:
        print("Error mounting Google Drive:", e)
        print("Please check your Google account login.")

mount_drive()

# U-Net Model for Crack Detection
def build_unet(input_shape=(256, 256, 3)):
    inputs = layers.Input(input_shape)
    
    # Encoder
    conv1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)
    conv1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)
    pool1 = layers.MaxPooling2D((2, 2))(conv1)
    
    conv2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)
    conv2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv2)
    pool2 = layers.MaxPooling2D((2, 2))(conv2)
    
    conv3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)
    conv3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)
    pool3 = layers.MaxPooling2D((2, 2))(conv3)
    
    # Bottleneck
    conv4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(pool3)
    conv4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(conv4)
    
    # Decoder
    up5 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv4)
    concat5 = layers.concatenate([up5, conv3])
    conv5 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(concat5)
    conv5 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conv5)
    
    up6 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv5)
    concat6 = layers.concatenate([up6, conv2])
    conv6 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(concat6)
    conv6 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv6)
    
    up7 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv6)
    concat7 = layers.concatenate([up7, conv1])
    conv7 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(concat7)
    conv7 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv7)
    
    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(conv7)
    
    model = models.Model(inputs, outputs)
    return model

# Load dataset (Example: Load images and masks from directories)
def load_data(image_dir, mask_dir, img_size=(256, 256)):
    images, masks = [], []
    if not os.path.exists(image_dir) or not os.path.exists(mask_dir):
        raise FileNotFoundError("Image or mask directory not found. Please check the path.")
    
    for file in os.listdir(image_dir):
        img_path = os.path.join(image_dir, file)
        mask_path = os.path.join(mask_dir, file)
        
        if not os.path.isfile(img_path) or not os.path.isfile(mask_path):
            print(f"Skipping {file} as corresponding image/mask is missing.")
            continue
        
        img = cv2.imread(img_path)
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
        
        if img is None or mask is None:
            print(f"Skipping {file} due to loading error.")
            continue
        
        img = cv2.resize(img, img_size)
        mask = cv2.resize(mask, img_size)
        mask = np.expand_dims(mask, axis=-1)
        
        images.append(img)
        masks.append(mask)
    
    images = np.array(images) / 255.0  # Normalize images
    masks = np.array(masks) / 255.0    # Normalize masks
    return images, masks

# Define paths (Update these paths)
image_dir = "/content/drive/MyDrive/your_dataset/images"
mask_dir = "/content/drive/MyDrive/your_dataset/masks"

# Check if paths are correct
if not os.path.exists(image_dir) or not os.path.exists(mask_dir):
    raise FileNotFoundError("Dataset directories not found. Verify the paths in Google Drive.")

images, masks = load_data(image_dir, mask_dir)

if len(images) == 0 or len(masks) == 0:
    raise ValueError("No valid images or masks found. Check dataset paths and contents.")

# Split data
X_train, X_test, y_train, y_test = train_test_split(images, masks, test_size=0.2, random_state=42)

# Build and compile model
model = build_unet()
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train model
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=8)

# Save model
model.save("crack_detection_unet.h5")
